{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Data Processing for RAG with Data Prep Kit</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15976e3",
   "metadata": {},
   "source": [
    "## Before Running the notebook\n",
    "\n",
    "Please complete [setting up python dev environment](./setup-python-dev-env.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will process PDF documents as part of RAG pipeline\n",
    "\n",
    "![](media/rag-overview-2.png)\n",
    "\n",
    "This notebook will perform steps 1, 2 and 3 in RAG pipeline.\n",
    "\n",
    "Here are the processing steps:\n",
    "\n",
    "- **pdf2parquet** : Extract text from PDF and convert them into parquet files\n",
    "- **Chunk documents**: Split the PDFs into 'meaningful sections' (paragraphs, sentences ..etc)\n",
    "- **Exact Dedup**: Chunks with exact same content are filtered out\n",
    "- **Doc_ID generation**: Each chunk is assigned a uniq id, based on content and hash\n",
    "- **Fuzzy Dedup**: Eliminate chunks that are 'very similar' content\n",
    "- **Doc quality**: Scores the documents based on criteria like number of words, if it contains bad words ..etc\n",
    "- **Text encoder**: Convert chunks into vectors using embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b10be1",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33345487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "## Configuration\n",
    "class MyConfig:\n",
    "    pass \n",
    "\n",
    "MY_CONFIG = MyConfig ()\n",
    "\n",
    "## Input Data - configure this to the folder we want to process\n",
    "MY_CONFIG.INPUT_DATA_DIR = \"input\"\n",
    "MY_CONFIG.OUTPUT_FOLDER = \"output\"\n",
    "MY_CONFIG.OUTPUT_FOLDER_FINAL = os.path.join(MY_CONFIG.OUTPUT_FOLDER , \"output_final\")\n",
    "\n",
    "## Embedding model\n",
    "MY_CONFIG.EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "## RAY CONFIGURATION\n",
    "num_cpus_available =  os.cpu_count()\n",
    "# print (num_cpus_available)\n",
    "# MY_CONFIG.RAY_NUM_CPUS = num_cpus_available // 2  ## use half the available cores for processing\n",
    "MY_CONFIG.RAY_NUM_CPUS =  1\n",
    "# print (MY_CONFIG.RAY_NUM_CPUS)\n",
    "MY_CONFIG.RAY_MEMORY_GB = 2  # GB\n",
    "# MY_CONFIG.RAY_RUNTIME_WORKERS = num_cpus_available // 3\n",
    "MY_CONFIG.RAY_RUNTIME_WORKERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc3f0e",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "We will use [Walmart annual report PDFs](https://github.com/sujee/data/tree/main/data-prep-kit/walmart-reports-1) as our input data.\n",
    "\n",
    "Feel free to substitute your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c1ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local file 'input/Walmart-10K-Reports-Optimized_2023.pdf' (1.61 MB) already exists. Skipping download.\n",
      "Local file 'input/Walmart_2024.pdf' (4.87 MB) already exists. Skipping download.\n",
      "Local file 'input/Walmart_2024_copy.pdf' (4.87 MB) already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "from utils import download_file\n",
    "\n",
    "## Download the data files\n",
    "shutil.os.makedirs(MY_CONFIG.INPUT_DATA_DIR, exist_ok=True)\n",
    "\n",
    "download_file (url = 'https://raw.githubusercontent.com/sujee/data/main/data-prep-kit/walmart-reports-1/Walmart-10K-Reports-Optimized_2023.pdf', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'Walmart-10K-Reports-Optimized_2023.pdf' ))\n",
    "\n",
    "download_file (url = 'https://raw.githubusercontent.com/sujee/data/main/data-prep-kit/walmart-reports-1/Walmart_2024.pdf', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'Walmart_2024.pdf' ))\n",
    "\n",
    "download_file (url = 'https://raw.githubusercontent.com/sujee/data/main/data-prep-kit/walmart-reports-1/Walmart_2024.pdf', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'Walmart_2024_copy.pdf' ))  # create a dupe file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleared output directory\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(MY_CONFIG.INPUT_DATA_DIR ):\n",
    "    raise Exception (f\"‚ùå Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
    "\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print (\"‚úÖ Cleared output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d976e-cb4c-4469-af39-4b7ea507e9d8",
   "metadata": {},
   "source": [
    "### Import Common python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66178913-42b8-426b-a2e9-9587268fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Main repo root\n",
    "from utils import rootdir\n",
    "\n",
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "STAGE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "<a id=\"pdf2parquet\"></a>\n",
    "\n",
    "## Step-2: pdf2parquet -  Convert data from PDF to Parquet\n",
    "\n",
    "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
    "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-1: Processing input='input' --> output='output/01_parquet_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  += 1\n",
    "# STAGE = 1  ## DEBUG\n",
    "\n",
    "input_folder = MY_CONFIG.INPUT_DATA_DIR\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_parquet_out\")\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:00:45 INFO - Running locally\n",
      "01:00:45 INFO - pdf2parquet parameters are : {'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.JSON: 'application/json'>, 'do_table_structure': True, 'do_ocr': True, 'double_precision': 8}\n",
      "01:00:45 INFO - data factory data_ is using local data access: input_folder - input output_folder - output/01_parquet_out\n",
      "01:00:45 INFO - data factory data_ max_files -1, n_sample -1\n",
      "01:00:45 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "01:00:45 INFO - pipeline id pipeline_id\n",
      "01:00:45 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'}\n",
      "01:00:45 INFO - number of workers 2 worker options {'num_cpus': 1, 'memory': 2147483648, 'max_restarts': -1}\n",
      "01:00:45 INFO - actor creation delay 0\n",
      "01:00:45 INFO - job details {'job category': 'preprocessing', 'job name': 'pdf2parquet', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-09-11 01:00:47,416\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=256007)\u001b[0m 01:00:50 INFO - orchestrator started at 2024-09-11 01:00:50\n",
      "\u001b[36m(orchestrate pid=256007)\u001b[0m 01:00:50 INFO - Number of files is 3, source profile {'max_file_size': 4.640201568603516, 'min_file_size': 1.5370569229125977, 'total_file_size': 10.817460060119629}\n",
      "\u001b[36m(orchestrate pid=256007)\u001b[0m 01:00:50 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 13.172847747802734, 'object_store': 6.586423873901367}\n",
      "\u001b[36m(orchestrate pid=256007)\u001b[0m 01:00:50 INFO - Number of workers - 2 with {'num_cpus': 1, 'memory': 2147483648, 'max_restarts': -1} each\n",
      "\u001b[36m(RayTransformFileProcessor pid=256863)\u001b[0m 01:00:52 INFO - Initializing models\n",
      "Fetching 7 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 152917.33it/s]\n",
      "\u001b[36m(RayTransformFileProcessor pid=256863)\u001b[0m Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "\u001b[36m(RayTransformFileProcessor pid=256863)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-6-src-dev/lib/python3.11/site-packages/easyocr/detection.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(RayTransformFileProcessor pid=256863)\u001b[0m   net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "\u001b[36m(RayTransformFileProcessor pid=256863)\u001b[0m   state_dict = torch.load(model_path, map_location=device)\n",
      "\u001b[36m(RayTransformFileProcessor pid=256863)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-6-src-dev/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "\u001b[36m(RayTransformFileProcessor pid=256863)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "\u001b[36m(RayTransformFileProcessor pid=256863)\u001b[0m 2024-09-11 01:08:31.295 ( 460.293s) [        F9337740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "\u001b[36m(RayTransformFileProcessor pid=256864)\u001b[0m 01:00:52 INFO - Initializing models\n",
      "Fetching 7 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 27337.18it/s]\n",
      "\u001b[36m(RayTransformFileProcessor pid=256864)\u001b[0m Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "\u001b[36m(RayTransformFileProcessor pid=256864)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-6-src-dev/lib/python3.11/site-packages/easyocr/recognition.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTransformFileProcessor pid=256864)\u001b[0m   net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "\u001b[36m(RayTransformFileProcessor pid=256864)\u001b[0m   state_dict = torch.load(model_path, map_location=device)\n",
      "\u001b[36m(RayTransformFileProcessor pid=256864)\u001b[0m /home/sujee/apps/anaconda3/envs/data-prep-kit-6-src-dev/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "\u001b[36m(RayTransformFileProcessor pid=256864)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "\u001b[36m(orchestrate pid=256007)\u001b[0m 01:08:31 INFO - Completed 1 files in 7.694344302018483 min\n",
      "\u001b[36m(orchestrate pid=256007)\u001b[0m 01:08:31 INFO - Completed 1 files (33.333333333333336%)  in 7.694346678256989 min. Waiting for completion\n",
      "\u001b[36m(RayTransformFileProcessor pid=256864)\u001b[0m 2024-09-11 01:09:01.892 ( 490.867s) [        3DED5740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-selected\n",
      "\u001b[36m(RayTransformFileProcessor pid=256864)\u001b[0m 2024-09-11 01:09:01.892 ( 490.867s) [        3DED5740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-selected\n",
      "\u001b[36m(RayTransformFileProcessor pid=256864)\u001b[0m 2024-09-11 01:09:01.892 ( 490.867s) [        3DED5740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTransformFileProcessor pid=256863)\u001b[0m 2024-09-11 01:15:32.712 ( 881.710s) [        F9337740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "\u001b[36m(orchestrate pid=256007)\u001b[0m 01:15:33 INFO - Completed processing 3 files in 14.71947247982025 min\n",
      "\u001b[36m(orchestrate pid=256007)\u001b[0m 01:15:33 INFO - done flushing in 0.001501321792602539 sec\n",
      "01:15:43 INFO - Completed execution in 14.96568330526352 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:1 completed successfully\n",
      "CPU times: user 4.4 s, sys: 3.02 s, total: 7.41 s\n",
      "Wall time: 15min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pdf2parquet_transform import (\n",
    "    pdf2parquet_contents_type_cli_param,\n",
    "    pdf2parquet_contents_types,\n",
    ")\n",
    "from pdf2parquet_transform_python import Pdf2ParquetPythonTransformConfiguration\n",
    "from pdf2parquet_transform_ray import Pdf2ParquetRayTransformConfiguration\n",
    "\n",
    "from data_processing.utils import GB, ParamsUtils\n",
    "\n",
    "\n",
    "# create parameters\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS, \"memory\": MY_CONFIG.RAY_MEMORY_GB * GB}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "ingest_config = {\n",
    "    pdf2parquet_contents_type_cli_param: pdf2parquet_contents_types.JSON,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    \"data_files_to_use\": ast.literal_eval(\"['.pdf']\"),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(Pdf2ParquetRayTransformConfiguration())\n",
    "# launcher = PythonTransformLauncher(Pdf2ParquetPythonTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca790e0",
   "metadata": {},
   "source": [
    "### Inspect Generated output\n",
    "\n",
    "Here we should see one entry per input file processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe59563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensions (rows x columns)=  (3, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>df0d7842-b162-4511-80d9-190cb557bc34</td>\n",
       "      <td>pdf</td>\n",
       "      <td>cd408a97e6d67a9044e6992b6d0c9f553b9522e4b18123...</td>\n",
       "      <td>1215210</td>\n",
       "      <td>2024-09-11T01:08:31.867666</td>\n",
       "      <td>456.552947</td>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>100</td>\n",
       "      <td>82</td>\n",
       "      <td>1158</td>\n",
       "      <td>a1d5b15f-fe07-4444-a16f-f1a7b6fecff5</td>\n",
       "      <td>pdf</td>\n",
       "      <td>56eb1501e8cc4a4cb452a7457049c3184e994d35e67086...</td>\n",
       "      <td>1255786</td>\n",
       "      <td>2024-09-11T01:09:02.389041</td>\n",
       "      <td>487.067450</td>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>86f5aff5-ffd1-4a24-b67d-5f340529b925</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...</td>\n",
       "      <td>1215215</td>\n",
       "      <td>2024-09-11T01:15:33.376308</td>\n",
       "      <td>421.475128</td>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  \\\n",
       "0                        Walmart_2024.pdf   \n",
       "1  Walmart-10K-Reports-Optimized_2023.pdf   \n",
       "2                   Walmart_2024_copy.pdf   \n",
       "\n",
       "                                            contents  num_pages  num_tables  \\\n",
       "0  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...        100          83   \n",
       "1  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...        100          82   \n",
       "2  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...        100          83   \n",
       "\n",
       "   num_doc_elements                           document_id  ext  \\\n",
       "0              1058  df0d7842-b162-4511-80d9-190cb557bc34  pdf   \n",
       "1              1158  a1d5b15f-fe07-4444-a16f-f1a7b6fecff5  pdf   \n",
       "2              1058  86f5aff5-ffd1-4a24-b67d-5f340529b925  pdf   \n",
       "\n",
       "                                                hash     size  \\\n",
       "0  cd408a97e6d67a9044e6992b6d0c9f553b9522e4b18123...  1215210   \n",
       "1  56eb1501e8cc4a4cb452a7457049c3184e994d35e67086...  1255786   \n",
       "2  f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...  1215215   \n",
       "\n",
       "                date_acquired  pdf_convert_time  \\\n",
       "0  2024-09-11T01:08:31.867666        456.552947   \n",
       "1  2024-09-11T01:09:02.389041        487.067450   \n",
       "2  2024-09-11T01:15:33.376308        421.475128   \n",
       "\n",
       "                          source_filename  \n",
       "0                        Walmart_2024.pdf  \n",
       "1  Walmart-10K-Reports-Optimized_2023.pdf  \n",
       "2                   Walmart_2024_copy.pdf  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Output dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.head(5)\n",
    "\n",
    "## To display certain columns\n",
    "#parquet_df[['column1', 'column2', 'column3']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72274586",
   "metadata": {},
   "source": [
    "<a id=\"chunking\"></a>\n",
    "\n",
    "##  Step-3: Doc chunks\n",
    "\n",
    "Split the documents in chunks, according to their layout segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96198fa6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305f00a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-2: Processing input='output/01_parquet_out' --> output='output/02_chunk_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  += 1\n",
    "# STAGE = 2  ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_chunk_out\")\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f2cd1",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b7b18d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:15:45 INFO - Running locally\n",
      "01:15:45 INFO - doc_chunk parameters are : {'chunking_type': <chunking_types.DL_JSON: 'dl_json'>, 'content_column_name': 'contents', 'output_chunk_column_name': 'contents', 'output_jsonpath_column_name': 'doc_jsonpath', 'output_pageno_column_name': 'page_number', 'output_bbox_column_name': 'bbox'}\n",
      "01:15:45 INFO - data factory data_ is using local data access: input_folder - output/01_parquet_out output_folder - output/02_chunk_out\n",
      "01:15:45 INFO - data factory data_ max_files -1, n_sample -1\n",
      "01:15:45 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "01:15:45 INFO - pipeline id pipeline_id\n",
      "01:15:45 INFO - code location None\n",
      "01:15:45 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "01:15:45 INFO - actor creation delay 0\n",
      "01:15:45 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_chunk', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-09-11 01:15:47,860\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=270113)\u001b[0m 01:15:50 INFO - orchestrator started at 2024-09-11 01:15:50\n",
      "\u001b[36m(orchestrate pid=270113)\u001b[0m 01:15:50 INFO - Number of files is 3, source profile {'max_file_size': 0.3882484436035156, 'min_file_size': 0.3697977066040039, 'total_file_size': 1.1278619766235352}\n",
      "\u001b[36m(orchestrate pid=270113)\u001b[0m 01:15:50 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 13.263294983655214, 'object_store': 6.631647490896285}\n",
      "\u001b[36m(orchestrate pid=270113)\u001b[0m 01:15:50 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=270113)\u001b[0m 01:15:51 INFO - Completed 1 files in 0.02923989693323771 min\n",
      "\u001b[36m(orchestrate pid=270113)\u001b[0m 01:15:51 INFO - Completed 1 files (33.333333333333336%)  in 0.029241561889648438 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=270113)\u001b[0m 01:15:51 INFO - Completed processing 3 files in 0.03176183700561523 min\n",
      "\u001b[36m(orchestrate pid=270113)\u001b[0m 01:15:51 INFO - done flushing in 0.0014557838439941406 sec\n",
      "01:16:01 INFO - Completed execution in 0.26794455846150717 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:2 completed successfully\n",
      "CPU times: user 893 ms, sys: 229 ms, total: 1.12 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Import doc_json_chunk transform configuration\n",
    "from doc_chunk_transform_ray import DocChunkRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # doc_chunk arguments\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(DocChunkRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213afdf6",
   "metadata": {},
   "source": [
    "### Inspect Generated output\n",
    "\n",
    "We would see documents are split into many chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8138d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed : 3\n",
      "Chunks created : 1,973\n",
      "Input data dimensions (rows x columns)=  (3, 12)\n",
      "Output data dimensions (rows x columns)=  (1973, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>86f5aff5-ffd1-4a24-b67d-5f340529b925</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...</td>\n",
       "      <td>1215215</td>\n",
       "      <td>2024-09-11T01:15:33.376308</td>\n",
       "      <td>421.475128</td>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>( ' , ( % # - , . % - , ( ) + - # ( ' ,\\n( 9 H...</td>\n",
       "      <td>$.tables[17]</td>\n",
       "      <td>45</td>\n",
       "      <td>[35.58596802, 645.70812988, 549.18774414, 724....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>86f5aff5-ffd1-4a24-b67d-5f340529b925</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...</td>\n",
       "      <td>1215215</td>\n",
       "      <td>2024-09-11T01:15:33.376308</td>\n",
       "      <td>421.475128</td>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td># + % . , . + &amp; ' -\\n. &lt; 9 C A D 5 B M F 9 7 C...</td>\n",
       "      <td>$.main-text[642]</td>\n",
       "      <td>63</td>\n",
       "      <td>[35.25471497, 124.5, 550.93585205, 185.64001465]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>86f5aff5-ffd1-4a24-b67d-5f340529b925</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...</td>\n",
       "      <td>1215215</td>\n",
       "      <td>2024-09-11T01:15:33.376308</td>\n",
       "      <td>421.475128</td>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>% @ E 6 * 9 @ C E E 6 C &gt; @ C C @ H : ? 8 D 2 ...</td>\n",
       "      <td>$.main-text[742]</td>\n",
       "      <td>71</td>\n",
       "      <td>[35.35443878, 679.0, 549.61694336, 704.14001465]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>86f5aff5-ffd1-4a24-b67d-5f340529b925</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...</td>\n",
       "      <td>1215215</td>\n",
       "      <td>2024-09-11T01:15:33.376308</td>\n",
       "      <td>421.475128</td>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>F &gt; 2 ? 2 A : E 2 = $ 2 ? 2 8 6 &gt; 6 ? E\\n. 9 7...</td>\n",
       "      <td>$.main-text[182]</td>\n",
       "      <td>15</td>\n",
       "      <td>[35.34682083, 335.99993896, 540.63574219, 371....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>df0d7842-b162-4511-80d9-190cb557bc34</td>\n",
       "      <td>pdf</td>\n",
       "      <td>cd408a97e6d67a9044e6992b6d0c9f553b9522e4b18123...</td>\n",
       "      <td>1215210</td>\n",
       "      <td>2024-09-11T01:08:31.867666</td>\n",
       "      <td>456.552947</td>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td># 2 ( * 6 8 &amp; . 2 \" &amp; &lt; 3 7 . 8 . 3 2 7\\n. &lt; 9...</td>\n",
       "      <td>$.main-text[820]</td>\n",
       "      <td>76</td>\n",
       "      <td>[35.27887726, 296.0, 549.15588379, 321.14001465]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>86f5aff5-ffd1-4a24-b67d-5f340529b925</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...</td>\n",
       "      <td>1215215</td>\n",
       "      <td>2024-09-11T01:15:33.376308</td>\n",
       "      <td>421.475128</td>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>+ $ ) + &amp; ) * / , + - &amp; ) * % &amp; ) ' &amp; ) + &amp; - ...</td>\n",
       "      <td>$.main-text[914]</td>\n",
       "      <td>84</td>\n",
       "      <td>[35.42567444, 631.0, 548.65386963, 668.14001465]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>df0d7842-b162-4511-80d9-190cb557bc34</td>\n",
       "      <td>pdf</td>\n",
       "      <td>cd408a97e6d67a9044e6992b6d0c9f553b9522e4b18123...</td>\n",
       "      <td>1215210</td>\n",
       "      <td>2024-09-11T01:08:31.867666</td>\n",
       "      <td>456.552947</td>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>% @ E 6 * 9 @ C E E 6 C &gt; @ C C @ H : ? 8 D 2 ...</td>\n",
       "      <td>$.main-text[745]</td>\n",
       "      <td>71</td>\n",
       "      <td>[35.17734146, 515.39996338, 542.61682129, 588....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>82</td>\n",
       "      <td>1158</td>\n",
       "      <td>a1d5b15f-fe07-4444-a16f-f1a7b6fecff5</td>\n",
       "      <td>pdf</td>\n",
       "      <td>56eb1501e8cc4a4cb452a7457049c3184e994d35e67086...</td>\n",
       "      <td>1255786</td>\n",
       "      <td>2024-09-11T01:09:02.389041</td>\n",
       "      <td>487.067450</td>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>As of January 31,\\nThe aggregate annual lease ...</td>\n",
       "      <td>$.main-text[802]</td>\n",
       "      <td>73</td>\n",
       "      <td>[46.61784363, 518.88317871, 334.92474365, 530....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>df0d7842-b162-4511-80d9-190cb557bc34</td>\n",
       "      <td>pdf</td>\n",
       "      <td>cd408a97e6d67a9044e6992b6d0c9f553b9522e4b18123...</td>\n",
       "      <td>1215210</td>\n",
       "      <td>2024-09-11T01:08:31.867666</td>\n",
       "      <td>456.552947</td>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>* F &gt; &gt; 2 C J @ 7 C : E : 4 2 = 4 4 @ F ? E : ...</td>\n",
       "      <td>$.main-text[520]</td>\n",
       "      <td>50</td>\n",
       "      <td>[35.10198975, 586.0, 544.31689453, 623.14001465]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>86f5aff5-ffd1-4a24-b67d-5f340529b925</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...</td>\n",
       "      <td>1215215</td>\n",
       "      <td>2024-09-11T01:15:33.376308</td>\n",
       "      <td>421.475128</td>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>= F 4 E F 2 E : @ ? D : ? 7 @ C 6 : 8 ? 6 I 4 ...</td>\n",
       "      <td>$.main-text[263]</td>\n",
       "      <td>25</td>\n",
       "      <td>[35.24409485, 64.00000763, 548.74487305, 125.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1973 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename  num_pages  num_tables  \\\n",
       "1599                   Walmart_2024_copy.pdf        100          83   \n",
       "1735                   Walmart_2024_copy.pdf        100          83   \n",
       "1796                   Walmart_2024_copy.pdf        100          83   \n",
       "1384                   Walmart_2024_copy.pdf        100          83   \n",
       "545                         Walmart_2024.pdf        100          83   \n",
       "...                                      ...        ...         ...   \n",
       "1912                   Walmart_2024_copy.pdf        100          83   \n",
       "500                         Walmart_2024.pdf        100          83   \n",
       "1136  Walmart-10K-Reports-Optimized_2023.pdf        100          82   \n",
       "353                         Walmart_2024.pdf        100          83   \n",
       "1461                   Walmart_2024_copy.pdf        100          83   \n",
       "\n",
       "      num_doc_elements                           document_id  ext  \\\n",
       "1599              1058  86f5aff5-ffd1-4a24-b67d-5f340529b925  pdf   \n",
       "1735              1058  86f5aff5-ffd1-4a24-b67d-5f340529b925  pdf   \n",
       "1796              1058  86f5aff5-ffd1-4a24-b67d-5f340529b925  pdf   \n",
       "1384              1058  86f5aff5-ffd1-4a24-b67d-5f340529b925  pdf   \n",
       "545               1058  df0d7842-b162-4511-80d9-190cb557bc34  pdf   \n",
       "...                ...                                   ...  ...   \n",
       "1912              1058  86f5aff5-ffd1-4a24-b67d-5f340529b925  pdf   \n",
       "500               1058  df0d7842-b162-4511-80d9-190cb557bc34  pdf   \n",
       "1136              1158  a1d5b15f-fe07-4444-a16f-f1a7b6fecff5  pdf   \n",
       "353               1058  df0d7842-b162-4511-80d9-190cb557bc34  pdf   \n",
       "1461              1058  86f5aff5-ffd1-4a24-b67d-5f340529b925  pdf   \n",
       "\n",
       "                                                   hash     size  \\\n",
       "1599  f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...  1215215   \n",
       "1735  f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...  1215215   \n",
       "1796  f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...  1215215   \n",
       "1384  f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...  1215215   \n",
       "545   cd408a97e6d67a9044e6992b6d0c9f553b9522e4b18123...  1215210   \n",
       "...                                                 ...      ...   \n",
       "1912  f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...  1215215   \n",
       "500   cd408a97e6d67a9044e6992b6d0c9f553b9522e4b18123...  1215210   \n",
       "1136  56eb1501e8cc4a4cb452a7457049c3184e994d35e67086...  1255786   \n",
       "353   cd408a97e6d67a9044e6992b6d0c9f553b9522e4b18123...  1215210   \n",
       "1461  f9a875d8dfe00abad97c66a0be7d501f01ad8173b3da33...  1215215   \n",
       "\n",
       "                   date_acquired  pdf_convert_time  \\\n",
       "1599  2024-09-11T01:15:33.376308        421.475128   \n",
       "1735  2024-09-11T01:15:33.376308        421.475128   \n",
       "1796  2024-09-11T01:15:33.376308        421.475128   \n",
       "1384  2024-09-11T01:15:33.376308        421.475128   \n",
       "545   2024-09-11T01:08:31.867666        456.552947   \n",
       "...                          ...               ...   \n",
       "1912  2024-09-11T01:15:33.376308        421.475128   \n",
       "500   2024-09-11T01:08:31.867666        456.552947   \n",
       "1136  2024-09-11T01:09:02.389041        487.067450   \n",
       "353   2024-09-11T01:08:31.867666        456.552947   \n",
       "1461  2024-09-11T01:15:33.376308        421.475128   \n",
       "\n",
       "                             source_filename  \\\n",
       "1599                   Walmart_2024_copy.pdf   \n",
       "1735                   Walmart_2024_copy.pdf   \n",
       "1796                   Walmart_2024_copy.pdf   \n",
       "1384                   Walmart_2024_copy.pdf   \n",
       "545                         Walmart_2024.pdf   \n",
       "...                                      ...   \n",
       "1912                   Walmart_2024_copy.pdf   \n",
       "500                         Walmart_2024.pdf   \n",
       "1136  Walmart-10K-Reports-Optimized_2023.pdf   \n",
       "353                         Walmart_2024.pdf   \n",
       "1461                   Walmart_2024_copy.pdf   \n",
       "\n",
       "                                               contents      doc_jsonpath  \\\n",
       "1599  ( ' , ( % # - , . % - , ( ) + - # ( ' ,\\n( 9 H...      $.tables[17]   \n",
       "1735  # + % . , . + & ' -\\n. < 9 C A D 5 B M F 9 7 C...  $.main-text[642]   \n",
       "1796  % @ E 6 * 9 @ C E E 6 C > @ C C @ H : ? 8 D 2 ...  $.main-text[742]   \n",
       "1384  F > 2 ? 2 A : E 2 = $ 2 ? 2 8 6 > 6 ? E\\n. 9 7...  $.main-text[182]   \n",
       "545   # 2 ( * 6 8 & . 2 \" & < 3 7 . 8 . 3 2 7\\n. < 9...  $.main-text[820]   \n",
       "...                                                 ...               ...   \n",
       "1912  + $ ) + & ) * / , + - & ) * % & ) ' & ) + & - ...  $.main-text[914]   \n",
       "500   % @ E 6 * 9 @ C E E 6 C > @ C C @ H : ? 8 D 2 ...  $.main-text[745]   \n",
       "1136  As of January 31,\\nThe aggregate annual lease ...  $.main-text[802]   \n",
       "353   * F > > 2 C J @ 7 C : E : 4 2 = 4 4 @ F ? E : ...  $.main-text[520]   \n",
       "1461  = F 4 E F 2 E : @ ? D : ? 7 @ C 6 : 8 ? 6 I 4 ...  $.main-text[263]   \n",
       "\n",
       "      page_number                                               bbox  \n",
       "1599           45  [35.58596802, 645.70812988, 549.18774414, 724....  \n",
       "1735           63   [35.25471497, 124.5, 550.93585205, 185.64001465]  \n",
       "1796           71   [35.35443878, 679.0, 549.61694336, 704.14001465]  \n",
       "1384           15  [35.34682083, 335.99993896, 540.63574219, 371....  \n",
       "545            76   [35.27887726, 296.0, 549.15588379, 321.14001465]  \n",
       "...           ...                                                ...  \n",
       "1912           84   [35.42567444, 631.0, 548.65386963, 668.14001465]  \n",
       "500            71  [35.17734146, 515.39996338, 542.61682129, 588....  \n",
       "1136           73  [46.61784363, 518.88317871, 334.92474365, 530....  \n",
       "353            50   [35.10198975, 586.0, 544.31689453, 623.14001465]  \n",
       "1461           25  [35.24409485, 64.00000763, 548.74487305, 125.1...  \n",
       "\n",
       "[1973 rows x 15 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (f\"Files processed : {input_df.shape[0]:,}\")\n",
    "print (f\"Chunks created : {output_df.shape[0]:,}\")\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(max(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "## Step-4: Exact Dedup\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c7a1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-3: Processing input='output/02_chunk_out' --> output='output/03_ededupe_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  += 1\n",
    "# STAGE  = 3  ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_ededupe_out\")\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SnapshotUtils' from 'data_processing.data_access' (/home/sujee/apps/anaconda3/envs/data-prep-kit-6-src-dev/lib/python3.11/site-packages/data_processing/data_access/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/my-stuff/projects/ai-alliance/data-prep-kit-sujee/transforms/universal/ededup/ray/src/ededup_transform_ray.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_processing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_access\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataAccessFactoryBase, SnapshotUtils\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_processing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformUtils, UnrecoverableException\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_processing_ray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     DefaultRayTransformRuntime,\n\u001b[1;32m     22\u001b[0m     RayTransformLauncher,\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SnapshotUtils' from 'data_processing.data_access' (/home/sujee/apps/anaconda3/envs/data-prep-kit-6-src-dev/lib/python3.11/site-packages/data_processing/data_access/__init__.py)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import ededup transform configuration\n",
    "from ededup_transform_ray import EdedupRayTransformRuntimeConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(EdedupRayTransformRuntimeConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1c3c3",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d824ebf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_parquet_files_as_df\n\u001b[0;32m----> 3\u001b[0m output_df \u001b[38;5;241m=\u001b[39m \u001b[43mread_parquet_files_as_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput data dimensions (rows x columns)= \u001b[39m\u001b[38;5;124m\"\u001b[39m, input_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput data dimensions (rows x columns)= \u001b[39m\u001b[38;5;124m\"\u001b[39m, output_df\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/my-stuff/projects/ai-alliance/data-prep-kit-sujee/examples/notebooks/rag/utils.py:23\u001b[0m, in \u001b[0;36mread_parquet_files_as_df\u001b[0;34m(parquet_dir)\u001b[0m\n\u001b[1;32m     20\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Concatenate all DataFrames into a single DataFrame\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m data_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_df\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/data-prep-kit-6-src-dev/lib/python3.11/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/data-prep-kit-6-src-dev/lib/python3.11/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/apps/anaconda3/envs/data-prep-kit-6-src-dev/lib/python3.11/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (f\"Input chunks before exact dedupe : {input_df.shape[0]:,}\")\n",
    "print (f\"Output chunks after exact dedupe : {output_df.shape[0]:,}\")\n",
    "print (\"Duplicate chunks removed :  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(max(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4d00-33bb-4d9a-9f34-4d7f3ee0b7bc",
   "metadata": {},
   "source": [
    "## Step-5:  DOC ID generation\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is a pre-requisite for fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f62394-fbde-495c-bbbb-83161b006bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input for this stage is the output of exact dedeup component\n",
    "# output of this component makes it possible for fdedup component to run on data.\n",
    "\n",
    "STAGE  += 1\n",
    "# STAGE  = 4  ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_doc_id_out\")\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6daf36d-686c-4e0a-aabf-ce55f999bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from doc_id_transform_ray import DocIDRayTransformRuntimeConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(DocIDRayTransformRuntimeConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d492c2b",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ade826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(max(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## Step-6: Fuzzy Dedup\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input to this component is the output of doc_id generator component. \n",
    "\n",
    "STAGE  += 1\n",
    "# STAGE  = 5  ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_fdedupe_out\")\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.5,\n",
    "    \"fdedup_doc_cpu\": 0.5,\n",
    "    \"fdedup_mhash_cpu\": 0.5,\n",
    "    \"fdedup_num_doc_actors\": 2,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 2,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8cd11",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (\"Duplicate chunks removed  by fuzzy-dedupe:  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(max(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646cbb7-3046-44c0-827d-d102d3ff7cb8",
   "metadata": {},
   "source": [
    "## Step-7: Document Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e985668-848b-4633-b0d8-9fe70ada0c91",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f080011-c9fe-430e-9ecc-f2220d2c8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE  += 1\n",
    "# STAGE  = 6 ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_doc_quality_out\")\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02982c5-f398-4a1a-a9fe-42d7ae748c7c",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29319fb9-b0d8-4f86-9bc5-b92960ad8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from doc_quality_transform import (\n",
    "    text_lang_cli_param,\n",
    "    doc_content_column_cli_param,\n",
    "    bad_word_filepath_cli_param,\n",
    ")\n",
    "from doc_quality_transform_ray import DocQualityRayTransformConfiguration\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "doc_quality_basedir = os.path.join(rootdir, \"transforms\", \"language\", \"doc_quality\", \"python\")\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_creation_delay\": 0,\n",
    "    # doc quality configuration\n",
    "    text_lang_cli_param: \"en\",\n",
    "    doc_content_column_cli_param: \"contents\",\n",
    "    bad_word_filepath_cli_param: os.path.join(doc_quality_basedir, \"ldnoobw\", \"en\"),\n",
    "}\n",
    "\n",
    "\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(DocQualityRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7d855",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f631d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(max(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## Step-8:   Text encoding\n",
    "\n",
    "Encode text for the vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE  += 1\n",
    "# STAGE  = 7 ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_encoder_out\")\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from text_encoder_transform_ray import TextEncoderRayTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # text_encoder\n",
    "    \"text_encoder_model_name\": MY_CONFIG.EMBEDDING_MODEL,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(TextEncoderRayTransformConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734852c",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(max(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12630-be6b-4188-a925-77117155617b",
   "metadata": {},
   "source": [
    "## Step-9: Copy output to final output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER_FINAL, ignore_errors=True)\n",
    "shutil.copytree(src=output_folder, dst=MY_CONFIG.OUTPUT_FOLDER_FINAL)\n",
    "\n",
    "print (f\"‚úÖ Copied output from '{output_folder}' --> '{MY_CONFIG.OUTPUT_FOLDER_FINAL}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
