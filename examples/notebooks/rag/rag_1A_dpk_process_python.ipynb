{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Data Processing for RAG with Data Prep Kit (Python)</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15976e3",
   "metadata": {},
   "source": [
    "## Before Running the notebook\n",
    "\n",
    "Please complete [setting up python dev environment](./setup-python-dev-env.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will process PDF documents as part of RAG pipeline\n",
    "\n",
    "![](media/rag-overview-2.png)\n",
    "\n",
    "This notebook will perform steps 1, 2 and 3 in RAG pipeline.\n",
    "\n",
    "Here are the processing steps:\n",
    "\n",
    "- **pdf2parquet** : Extract text from PDF and convert them into parquet files\n",
    "- **Chunk documents**: Split the PDFs into 'meaningful sections' (paragraphs, sentences ..etc)\n",
    "- **Exact Dedup**: Chunks with exact same content are filtered out\n",
    "- **Doc_ID generation**: Each chunk is assigned a uniq id, based on content and hash\n",
    "- **Fuzzy Dedup**: Eliminate chunks that are 'very similar' content\n",
    "- **Doc quality**: Scores the documents based on criteria like number of words, if it contains bad words ..etc\n",
    "- **Text encoder**: Convert chunks into vectors using embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b10be1",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33345487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "## Configuration\n",
    "class MyConfig:\n",
    "    pass \n",
    "\n",
    "MY_CONFIG = MyConfig ()\n",
    "\n",
    "## Input Data - configure this to the folder we want to process\n",
    "MY_CONFIG.INPUT_DATA_DIR = \"input\"\n",
    "MY_CONFIG.OUTPUT_FOLDER = \"output\"\n",
    "MY_CONFIG.OUTPUT_FOLDER_FINAL = os.path.join(MY_CONFIG.OUTPUT_FOLDER , \"output_final\")\n",
    "\n",
    "## Embedding model\n",
    "MY_CONFIG.EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "## RAY CONFIGURATION\n",
    "num_cpus_available =  os.cpu_count()\n",
    "# print (num_cpus_available)\n",
    "# MY_CONFIG.RAY_NUM_CPUS = num_cpus_available // 2  ## use half the available cores for processing\n",
    "MY_CONFIG.RAY_NUM_CPUS =  1\n",
    "# print (MY_CONFIG.RAY_NUM_CPUS)\n",
    "MY_CONFIG.RAY_MEMORY_GB = 2  # GB\n",
    "# MY_CONFIG.RAY_RUNTIME_WORKERS = num_cpus_available // 3\n",
    "MY_CONFIG.RAY_RUNTIME_WORKERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc3f0e",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "We will use [Walmart annual report PDFs](https://github.com/sujee/data/tree/main/data-prep-kit/walmart-reports-1) as our input data.\n",
    "\n",
    "Feel free to substitute your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c1ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local file 'input/Walmart-10K-Reports-Optimized_2023.pdf' (1.61 MB) already exists. Skipping download.\n",
      "Local file 'input/Walmart_2024.pdf' (4.87 MB) already exists. Skipping download.\n",
      "Local file 'input/Walmart_2024_copy.pdf' (4.87 MB) already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "from utils import download_file\n",
    "\n",
    "## Download the data files\n",
    "shutil.os.makedirs(MY_CONFIG.INPUT_DATA_DIR, exist_ok=True)\n",
    "\n",
    "download_file (url = 'https://raw.githubusercontent.com/sujee/data/main/data-prep-kit/walmart-reports-1/Walmart-10K-Reports-Optimized_2023.pdf', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'Walmart-10K-Reports-Optimized_2023.pdf' ))\n",
    "\n",
    "download_file (url = 'https://raw.githubusercontent.com/sujee/data/main/data-prep-kit/walmart-reports-1/Walmart_2024.pdf', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'Walmart_2024.pdf' ))\n",
    "\n",
    "download_file (url = 'https://raw.githubusercontent.com/sujee/data/main/data-prep-kit/walmart-reports-1/Walmart_2024.pdf', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'Walmart_2024_copy.pdf' ))  # create a dupe file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleared output directory\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(MY_CONFIG.INPUT_DATA_DIR ):\n",
    "    raise Exception (f\"‚ùå Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
    "\n",
    "output_parquet_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '01_parquet_out')\n",
    "output_chunk_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '02_chunk_out')\n",
    "output_exact_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '03_exact_dedupe_out')\n",
    "output_docid_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '04_docid_out')\n",
    "output_fuzzy_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '05_fuzzy_dedupe_out')\n",
    "output_doc_quality_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '06_doc_quality_out')\n",
    "output_embeddings_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '07_embeddings_out')\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print (\"‚úÖ Cleared output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d976e-cb4c-4469-af39-4b7ea507e9d8",
   "metadata": {},
   "source": [
    "### Import Common python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66178913-42b8-426b-a2e9-9587268fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Main repo root\n",
    "from utils import rootdir\n",
    "\n",
    "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
    "from data_processing.utils import ParamsUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "<a id=\"pdf2parquet\"></a>\n",
    "\n",
    "## Step-2: pdf2parquet -  Convert data from PDF to Parquet\n",
    "\n",
    "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
    "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-1: Processing input='input' --> output='output/01_parquet_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE = 1 \n",
    "\n",
    "input_folder = MY_CONFIG.INPUT_DATA_DIR\n",
    "output_folder =  output_parquet_dir\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:52:33 INFO - pdf2parquet parameters are : {'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.JSON: 'application/json'>, 'do_table_structure': True, 'do_ocr': True, 'double_precision': 8}\n",
      "23:52:33 INFO - pipeline id pipeline_id\n",
      "23:52:33 INFO - code location None\n",
      "23:52:33 INFO - data factory data_ is using local data access: input_folder - input output_folder - output/01_parquet_out\n",
      "23:52:33 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:52:33 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "23:52:33 INFO - orchestrator pdf2parquet started at 2024-09-15 23:52:33\n",
      "23:52:33 INFO - Number of files is 3, source profile {'max_file_size': 4.640201568603516, 'min_file_size': 1.5370569229125977, 'total_file_size': 10.817460060119629}\n",
      "23:52:33 INFO - Initializing models\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c7df3cd9144e438bfb5aac111ee70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 23:54:23.623 ( 112.335s) [        F132D740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "2024-09-15 23:54:23.623 ( 112.336s) [        F132D740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-selected\n",
      "2024-09-15 23:54:23.623 ( 112.336s) [        F132D740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "2024-09-15 23:54:23.623 ( 112.336s) [        F132D740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "2024-09-15 23:54:23.623 ( 112.336s) [        F132D740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-selected\n",
      "23:54:23 INFO - Completed 1 files (33.33%) in 1.778 min\n",
      "2024-09-15 23:56:08.187 ( 216.900s) [        F132D740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "23:56:08 INFO - Completed 2 files (66.67%) in 3.527 min\n",
      "2024-09-15 23:57:53.054 ( 321.767s) [        F132D740]    doc_normalisation.h:448   WARN| found new `other` type: checkbox-unselected\n",
      "23:57:53 INFO - Completed 3 files (100.0%) in 5.273 min\n",
      "23:57:53 INFO - Done processing 3 files, waiting for flush() completion.\n",
      "23:57:53 INFO - done flushing in 0.0 sec\n",
      "23:57:53 INFO - Completed execution in 5.338 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:1 completed successfully\n",
      "CPU times: user 13min 44s, sys: 3.62 s, total: 13min 47s\n",
      "Wall time: 5min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pdf2parquet_transform import (\n",
    "    pdf2parquet_contents_type_cli_param,\n",
    "    pdf2parquet_contents_types,\n",
    ")\n",
    "from pdf2parquet_transform_python import Pdf2ParquetPythonTransformConfiguration\n",
    "\n",
    "from data_processing.utils import GB, ParamsUtils\n",
    "\n",
    "\n",
    "# create parameters\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "ingest_config = {\n",
    "    pdf2parquet_contents_type_cli_param: pdf2parquet_contents_types.JSON,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    \"data_files_to_use\": ast.literal_eval(\"['.pdf']\"),\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
    "# create launcher\n",
    "launcher = PythonTransformLauncher(Pdf2ParquetPythonTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Job failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca790e0",
   "metadata": {},
   "source": [
    "### Inspect Generated output\n",
    "\n",
    "Here we should see one entry per input file processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe59563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensions (rows x columns)=  (3, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>100</td>\n",
       "      <td>82</td>\n",
       "      <td>1158</td>\n",
       "      <td>63d14368-4cbc-46e7-aede-f103cfdc5b5b</td>\n",
       "      <td>pdf</td>\n",
       "      <td>49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...</td>\n",
       "      <td>1255781</td>\n",
       "      <td>2024-09-15T23:54:23.967489</td>\n",
       "      <td>106.661048</td>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>5c1590b9-c09f-4842-8a67-02bbaf73e5df</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4f8daeb70531619b07ca64624526e7785e802643c653ea...</td>\n",
       "      <td>1215221</td>\n",
       "      <td>2024-09-15T23:57:53.629710</td>\n",
       "      <td>104.700134</td>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>d0f0c84e-8b9f-4c01-a1a5-65d3c675400e</td>\n",
       "      <td>pdf</td>\n",
       "      <td>29190b8923fb2153321dc52292e19b359a96c2e9974048...</td>\n",
       "      <td>1215216</td>\n",
       "      <td>2024-09-15T23:56:08.895449</td>\n",
       "      <td>104.766304</td>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  \\\n",
       "0  Walmart-10K-Reports-Optimized_2023.pdf   \n",
       "1                   Walmart_2024_copy.pdf   \n",
       "2                        Walmart_2024.pdf   \n",
       "\n",
       "                                            contents  num_pages  num_tables  \\\n",
       "0  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...        100          82   \n",
       "1  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...        100          83   \n",
       "2  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...        100          83   \n",
       "\n",
       "   num_doc_elements                           document_id  ext  \\\n",
       "0              1158  63d14368-4cbc-46e7-aede-f103cfdc5b5b  pdf   \n",
       "1              1058  5c1590b9-c09f-4842-8a67-02bbaf73e5df  pdf   \n",
       "2              1058  d0f0c84e-8b9f-4c01-a1a5-65d3c675400e  pdf   \n",
       "\n",
       "                                                hash     size  \\\n",
       "0  49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...  1255781   \n",
       "1  4f8daeb70531619b07ca64624526e7785e802643c653ea...  1215221   \n",
       "2  29190b8923fb2153321dc52292e19b359a96c2e9974048...  1215216   \n",
       "\n",
       "                date_acquired  pdf_convert_time  \\\n",
       "0  2024-09-15T23:54:23.967489        106.661048   \n",
       "1  2024-09-15T23:57:53.629710        104.700134   \n",
       "2  2024-09-15T23:56:08.895449        104.766304   \n",
       "\n",
       "                          source_filename  \n",
       "0  Walmart-10K-Reports-Optimized_2023.pdf  \n",
       "1                   Walmart_2024_copy.pdf  \n",
       "2                        Walmart_2024.pdf  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Output dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.head(5)\n",
    "\n",
    "## To display certain columns\n",
    "#parquet_df[['column1', 'column2', 'column3']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72274586",
   "metadata": {},
   "source": [
    "<a id=\"chunking\"></a>\n",
    "\n",
    "##  Step-3: Doc chunks\n",
    "\n",
    "Split the documents in chunks, according to their layout segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96198fa6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305f00a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-2: Processing input='output/01_parquet_out' --> output='output/02_chunk_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE = 2\n",
    "\n",
    "input_folder = output_parquet_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_chunk_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f2cd1",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b7b18d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:57:54 INFO - doc_chunk parameters are : {'chunking_type': <chunking_types.DL_JSON: 'dl_json'>, 'content_column_name': 'contents', 'output_chunk_column_name': 'contents', 'output_jsonpath_column_name': 'doc_jsonpath', 'output_pageno_column_name': 'page_number', 'output_bbox_column_name': 'bbox'}\n",
      "23:57:54 INFO - pipeline id pipeline_id\n",
      "23:57:54 INFO - code location None\n",
      "23:57:54 INFO - data factory data_ is using local data access: input_folder - output/01_parquet_out output_folder - output/02_chunk_out\n",
      "23:57:54 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:57:54 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:57:54 INFO - orchestrator doc_chunk started at 2024-09-15 23:57:54\n",
      "23:57:54 INFO - Number of files is 3, source profile {'max_file_size': 0.23883724212646484, 'min_file_size': 0.20527362823486328, 'total_file_size': 0.6494197845458984}\n",
      "23:57:54 INFO - Completed 1 files (33.33%) in 0.002 min\n",
      "23:57:55 INFO - Completed 2 files (66.67%) in 0.004 min\n",
      "23:57:55 INFO - Completed 3 files (100.0%) in 0.009 min\n",
      "23:57:55 INFO - Done processing 3 files, waiting for flush() completion.\n",
      "23:57:55 INFO - done flushing in 0.0 sec\n",
      "23:57:55 INFO - Completed execution in 0.009 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:2 completed successfully\n",
      "CPU times: user 1.46 s, sys: 86.1 ms, total: 1.55 s\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Import doc_json_chunk transform configuration\n",
    "from doc_chunk_transform_python import DocChunkPythonTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # doc_chunk arguments\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = PythonTransformLauncher(DocChunkPythonTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213afdf6",
   "metadata": {},
   "source": [
    "### Inspect Generated output\n",
    "\n",
    "We would see documents are split into many chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8138d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed : 3\n",
      "Chunks created : 1,973\n",
      "Input data dimensions (rows x columns)=  (3, 12)\n",
      "Output data dimensions (rows x columns)=  (1973, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>5c1590b9-c09f-4842-8a67-02bbaf73e5df</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4f8daeb70531619b07ca64624526e7785e802643c653ea...</td>\n",
       "      <td>1215221</td>\n",
       "      <td>2024-09-15T23:57:53.629710</td>\n",
       "      <td>104.700134</td>\n",
       "      <td>Walmart_2024_copy.pdf</td>\n",
       "      <td>% @ E 6 % 6 E ? 4 @ &gt; 6 ' 6 C @ &gt; &gt; @ ? * 9 2 ...</td>\n",
       "      <td>$.main-text[693]</td>\n",
       "      <td>67</td>\n",
       "      <td>[35.41247559, 377.0, 549.92785645, 438.14001465]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>d0f0c84e-8b9f-4c01-a1a5-65d3c675400e</td>\n",
       "      <td>pdf</td>\n",
       "      <td>29190b8923fb2153321dc52292e19b359a96c2e9974048...</td>\n",
       "      <td>1215216</td>\n",
       "      <td>2024-09-15T23:56:08.895449</td>\n",
       "      <td>104.766304</td>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>3 1 4 &amp; 2 = ! - &amp; 6 * * 4 9 6 ( - &amp; 7 * 6 3 , ...</td>\n",
       "      <td>$.main-text[503]</td>\n",
       "      <td>48</td>\n",
       "      <td>[35.38315201, 213.5, 534.5958252, 238.64001465]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>d0f0c84e-8b9f-4c01-a1a5-65d3c675400e</td>\n",
       "      <td>pdf</td>\n",
       "      <td>29190b8923fb2153321dc52292e19b359a96c2e9974048...</td>\n",
       "      <td>1215216</td>\n",
       "      <td>2024-09-15T23:56:08.895449</td>\n",
       "      <td>104.766304</td>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>/ ) . \" 6 * % \" * . &amp; 9 $ &amp; 2 4 * ' 9 4 ) \" 4\\...</td>\n",
       "      <td>$.main-text[1011]</td>\n",
       "      <td>98</td>\n",
       "      <td>[175.83938599, 391.23840332, 409.16744995, 407...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   filename  num_pages  num_tables  num_doc_elements  \\\n",
       "1090  Walmart_2024_copy.pdf        100          83              1058   \n",
       "1638       Walmart_2024.pdf        100          83              1058   \n",
       "1955       Walmart_2024.pdf        100          83              1058   \n",
       "\n",
       "                               document_id  ext  \\\n",
       "1090  5c1590b9-c09f-4842-8a67-02bbaf73e5df  pdf   \n",
       "1638  d0f0c84e-8b9f-4c01-a1a5-65d3c675400e  pdf   \n",
       "1955  d0f0c84e-8b9f-4c01-a1a5-65d3c675400e  pdf   \n",
       "\n",
       "                                                   hash     size  \\\n",
       "1090  4f8daeb70531619b07ca64624526e7785e802643c653ea...  1215221   \n",
       "1638  29190b8923fb2153321dc52292e19b359a96c2e9974048...  1215216   \n",
       "1955  29190b8923fb2153321dc52292e19b359a96c2e9974048...  1215216   \n",
       "\n",
       "                   date_acquired  pdf_convert_time        source_filename  \\\n",
       "1090  2024-09-15T23:57:53.629710        104.700134  Walmart_2024_copy.pdf   \n",
       "1638  2024-09-15T23:56:08.895449        104.766304       Walmart_2024.pdf   \n",
       "1955  2024-09-15T23:56:08.895449        104.766304       Walmart_2024.pdf   \n",
       "\n",
       "                                               contents       doc_jsonpath  \\\n",
       "1090  % @ E 6 % 6 E ? 4 @ > 6 ' 6 C @ > > @ ? * 9 2 ...   $.main-text[693]   \n",
       "1638  3 1 4 & 2 = ! - & 6 * * 4 9 6 ( - & 7 * 6 3 , ...   $.main-text[503]   \n",
       "1955  / ) . \" 6 * % \" * . & 9 $ & 2 4 * ' 9 4 ) \" 4\\...  $.main-text[1011]   \n",
       "\n",
       "      page_number                                               bbox  \n",
       "1090           67   [35.41247559, 377.0, 549.92785645, 438.14001465]  \n",
       "1638           48    [35.38315201, 213.5, 534.5958252, 238.64001465]  \n",
       "1955           98  [175.83938599, 391.23840332, 409.16744995, 407...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (f\"Files processed : {input_df.shape[0]:,}\")\n",
    "print (f\"Chunks created : {output_df.shape[0]:,}\")\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "## Step-4: Exact Dedup\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c7a1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-3: Processing input='output/02_chunk_out' --> output='output/03_exact_dedupe_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  = 3\n",
    "\n",
    "input_folder = output_chunk_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_exact_dedupe_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:57:55 INFO - exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'document_id', 'use_snapshot': False, 'snapshot_directory': None}\n",
      "23:57:55 INFO - pipeline id pipeline_id\n",
      "23:57:55 INFO - code location None\n",
      "23:57:55 INFO - data factory data_ is using local data access: input_folder - output/02_chunk_out output_folder - output/03_exact_dedupe_out\n",
      "23:57:55 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:57:55 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:57:55 INFO - orchestrator ededup started at 2024-09-15 23:57:55\n",
      "23:57:55 INFO - Number of files is 3, source profile {'max_file_size': 0.1514434814453125, 'min_file_size': 0.1461324691772461, 'total_file_size': 0.44895172119140625}\n",
      "23:57:55 INFO - Starting from the beginning\n",
      "23:57:55 INFO - Completed 1 files (33.33%) in 0.0 min\n",
      "23:57:55 INFO - Completed 2 files (66.67%) in 0.0 min\n",
      "23:57:55 INFO - Completed 3 files (100.0%) in 0.001 min\n",
      "23:57:55 INFO - Done processing 3 files, waiting for flush() completion.\n",
      "23:57:55 INFO - done flushing in 0.0 sec\n",
      "23:57:55 INFO - Completed execution in 0.001 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:3 completed successfully\n",
      "CPU times: user 46.8 ms, sys: 6.02 ms, total: 52.8 ms\n",
      "Wall time: 47.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import ededup transform configuration\n",
    "from ededup_transform_python import EdedupPythonTransformRuntimeConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # ededup parameters\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = PythonTransformLauncher(EdedupPythonTransformRuntimeConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1c3c3",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d824ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (1973, 15)\n",
      "Output data dimensions (rows x columns)=  (2, 16)\n",
      "Input chunks before exact dedupe : 1,973\n",
      "Output chunks after exact dedupe : 2\n",
      "Duplicate chunks removed :   1971\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>82</td>\n",
       "      <td>1158</td>\n",
       "      <td>63d14368-4cbc-46e7-aede-f103cfdc5b5b</td>\n",
       "      <td>pdf</td>\n",
       "      <td>49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...</td>\n",
       "      <td>1255781</td>\n",
       "      <td>2024-09-15T23:54:23.967489</td>\n",
       "      <td>106.661048</td>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>A message from our CEO\\n\"We started this new y...</td>\n",
       "      <td>$.main-text[6]</td>\n",
       "      <td>2</td>\n",
       "      <td>[29.09381294, 166.47808838, 500.76074219, 270....</td>\n",
       "      <td>[63d14368-4cbc-46e7-aede-f103cfdc5b5b, 63d1436...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>d0f0c84e-8b9f-4c01-a1a5-65d3c675400e</td>\n",
       "      <td>pdf</td>\n",
       "      <td>29190b8923fb2153321dc52292e19b359a96c2e9974048...</td>\n",
       "      <td>1215216</td>\n",
       "      <td>2024-09-15T23:56:08.895449</td>\n",
       "      <td>104.766304</td>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>A message from our CEO\\n\"At Walmart, we're a p...</td>\n",
       "      <td>$.main-text[6]</td>\n",
       "      <td>2</td>\n",
       "      <td>[30.18040848, 338.63272095, 526.46081543, 436....</td>\n",
       "      <td>[d0f0c84e-8b9f-4c01-a1a5-65d3c675400e, d0f0c84...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  num_pages  num_tables  \\\n",
       "0  Walmart-10K-Reports-Optimized_2023.pdf        100          82   \n",
       "1                        Walmart_2024.pdf        100          83   \n",
       "\n",
       "   num_doc_elements                           document_id  ext  \\\n",
       "0              1158  63d14368-4cbc-46e7-aede-f103cfdc5b5b  pdf   \n",
       "1              1058  d0f0c84e-8b9f-4c01-a1a5-65d3c675400e  pdf   \n",
       "\n",
       "                                                hash     size  \\\n",
       "0  49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...  1255781   \n",
       "1  29190b8923fb2153321dc52292e19b359a96c2e9974048...  1215216   \n",
       "\n",
       "                date_acquired  pdf_convert_time  \\\n",
       "0  2024-09-15T23:54:23.967489        106.661048   \n",
       "1  2024-09-15T23:56:08.895449        104.766304   \n",
       "\n",
       "                          source_filename  \\\n",
       "0  Walmart-10K-Reports-Optimized_2023.pdf   \n",
       "1                        Walmart_2024.pdf   \n",
       "\n",
       "                                            contents    doc_jsonpath  \\\n",
       "0  A message from our CEO\\n\"We started this new y...  $.main-text[6]   \n",
       "1  A message from our CEO\\n\"At Walmart, we're a p...  $.main-text[6]   \n",
       "\n",
       "   page_number                                               bbox  \\\n",
       "0            2  [29.09381294, 166.47808838, 500.76074219, 270....   \n",
       "1            2  [30.18040848, 338.63272095, 526.46081543, 436....   \n",
       "\n",
       "                                             removed  \n",
       "0  [63d14368-4cbc-46e7-aede-f103cfdc5b5b, 63d1436...  \n",
       "1  [d0f0c84e-8b9f-4c01-a1a5-65d3c675400e, d0f0c84...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (f\"Input chunks before exact dedupe : {input_df.shape[0]:,}\")\n",
    "print (f\"Output chunks after exact dedupe : {output_df.shape[0]:,}\")\n",
    "print (\"Duplicate chunks removed :  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4d00-33bb-4d9a-9f34-4d7f3ee0b7bc",
   "metadata": {},
   "source": [
    "## Step-5:  DOC ID generation\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is a pre-requisite for fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6f62394-fbde-495c-bbbb-83161b006bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-4: Processing input='output/03_exact_dedupe_out' --> output='output/04_docid_out'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input for this stage is the output of exact dedeup component\n",
    "# output of this component makes it possible for fdedup component to run on data.\n",
    "\n",
    "STAGE  = 4\n",
    "\n",
    "input_folder = output_exact_dedupe_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_docid_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6daf36d-686c-4e0a-aabf-ce55f999bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:57:55 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'hash_column', 'int_column': 'int_id_column', 'start_id': 0}\n",
      "23:57:55 INFO - pipeline id pipeline_id\n",
      "23:57:55 INFO - code location None\n",
      "23:57:55 INFO - data factory data_ is using local data access: input_folder - output/03_exact_dedupe_out output_folder - output/04_docid_out\n",
      "23:57:55 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:57:55 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:57:55 INFO - orchestrator doc_id started at 2024-09-15 23:57:55\n",
      "23:57:55 INFO - Number of files is 3, source profile {'max_file_size': 0.010014533996582031, 'min_file_size': 0.0033016204833984375, 'total_file_size': 0.02299976348876953}\n",
      "23:57:55 INFO - Completed 1 files (33.33%) in 0.0 min\n",
      "23:57:55 INFO - Completed 2 files (66.67%) in 0.0 min\n",
      "23:57:55 WARNING - table is empty, skipping processing\n",
      "23:57:55 INFO - Completed 3 files (100.0%) in 0.0 min\n",
      "23:57:55 INFO - Done processing 3 files, waiting for flush() completion.\n",
      "23:57:55 INFO - done flushing in 0.0 sec\n",
      "23:57:55 INFO - Completed execution in 0.0 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:4 completed successfully\n",
      "CPU times: user 19.6 ms, sys: 3 ms, total: 22.6 ms\n",
      "Wall time: 18.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from doc_id_transform_python import DocIDPythonTransformRuntimeConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = PythonTransformLauncher(DocIDPythonTransformRuntimeConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d492c2b",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ade826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (2, 16)\n",
      "Output data dimensions (rows x columns)=  (2, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>removed</th>\n",
       "      <th>hash_column</th>\n",
       "      <th>int_id_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>d0f0c84e-8b9f-4c01-a1a5-65d3c675400e</td>\n",
       "      <td>pdf</td>\n",
       "      <td>29190b8923fb2153321dc52292e19b359a96c2e9974048...</td>\n",
       "      <td>1215216</td>\n",
       "      <td>2024-09-15T23:56:08.895449</td>\n",
       "      <td>104.766304</td>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>A message from our CEO\\n\"At Walmart, we're a p...</td>\n",
       "      <td>$.main-text[6]</td>\n",
       "      <td>2</td>\n",
       "      <td>[30.18040848, 338.63272095, 526.46081543, 436....</td>\n",
       "      <td>[d0f0c84e-8b9f-4c01-a1a5-65d3c675400e, d0f0c84...</td>\n",
       "      <td>01b953276ba54dfa57da3f73dfcbbd26e62b634e8a3a41...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>82</td>\n",
       "      <td>1158</td>\n",
       "      <td>63d14368-4cbc-46e7-aede-f103cfdc5b5b</td>\n",
       "      <td>pdf</td>\n",
       "      <td>49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...</td>\n",
       "      <td>1255781</td>\n",
       "      <td>2024-09-15T23:54:23.967489</td>\n",
       "      <td>106.661048</td>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>A message from our CEO\\n\"We started this new y...</td>\n",
       "      <td>$.main-text[6]</td>\n",
       "      <td>2</td>\n",
       "      <td>[29.09381294, 166.47808838, 500.76074219, 270....</td>\n",
       "      <td>[63d14368-4cbc-46e7-aede-f103cfdc5b5b, 63d1436...</td>\n",
       "      <td>a651a5dc74053a1d5c92d134baf9ea23b7c009036a2619...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  num_pages  num_tables  \\\n",
       "1                        Walmart_2024.pdf        100          83   \n",
       "0  Walmart-10K-Reports-Optimized_2023.pdf        100          82   \n",
       "\n",
       "   num_doc_elements                           document_id  ext  \\\n",
       "1              1058  d0f0c84e-8b9f-4c01-a1a5-65d3c675400e  pdf   \n",
       "0              1158  63d14368-4cbc-46e7-aede-f103cfdc5b5b  pdf   \n",
       "\n",
       "                                                hash     size  \\\n",
       "1  29190b8923fb2153321dc52292e19b359a96c2e9974048...  1215216   \n",
       "0  49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...  1255781   \n",
       "\n",
       "                date_acquired  pdf_convert_time  \\\n",
       "1  2024-09-15T23:56:08.895449        104.766304   \n",
       "0  2024-09-15T23:54:23.967489        106.661048   \n",
       "\n",
       "                          source_filename  \\\n",
       "1                        Walmart_2024.pdf   \n",
       "0  Walmart-10K-Reports-Optimized_2023.pdf   \n",
       "\n",
       "                                            contents    doc_jsonpath  \\\n",
       "1  A message from our CEO\\n\"At Walmart, we're a p...  $.main-text[6]   \n",
       "0  A message from our CEO\\n\"We started this new y...  $.main-text[6]   \n",
       "\n",
       "   page_number                                               bbox  \\\n",
       "1            2  [30.18040848, 338.63272095, 526.46081543, 436....   \n",
       "0            2  [29.09381294, 166.47808838, 500.76074219, 270....   \n",
       "\n",
       "                                             removed  \\\n",
       "1  [d0f0c84e-8b9f-4c01-a1a5-65d3c675400e, d0f0c84...   \n",
       "0  [63d14368-4cbc-46e7-aede-f103cfdc5b5b, 63d1436...   \n",
       "\n",
       "                                         hash_column  int_id_column  \n",
       "1  01b953276ba54dfa57da3f73dfcbbd26e62b634e8a3a41...              1  \n",
       "0  a651a5dc74053a1d5c92d134baf9ea23b7c009036a2619...              0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## Step-6: Fuzzy Dedup\n",
    "\n",
    "**Fuzzy dedupe is currently available in RAY version**\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-5: Processing input='output/04_docid_out' --> output='output/05_fuzzy_dedupe_out'\n"
     ]
    }
   ],
   "source": [
    "## Input to this component is the output of doc_id generator component. \n",
    "\n",
    "STAGE  = 5\n",
    "\n",
    "input_folder = output_docid_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_fuzzy_dedupe_dir\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:57:55 INFO - Running locally\n",
      "23:57:55 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'int_id_column', 'cluster_column': 'hash_column', 'bucket_cpu': 0.3, 'mhash_cpu': 0.3, 'doc_cpu': 0.3, 'num_doc_actors': 1, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 1, 'num_permutations': 64, 'threshold': 0.8, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 1}}\n",
      "23:57:55 INFO - data factory data_ is using local data access: input_folder - output/04_docid_out output_folder - output/05_fuzzy_dedupe_out\n",
      "23:57:55 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:57:55 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:57:55 INFO - pipeline id pipeline_id\n",
      "23:57:55 INFO - code location None\n",
      "23:57:55 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "23:57:55 INFO - actor creation delay 0\n",
      "23:57:55 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-09-15 23:57:57,665\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - orchestrator started at 2024-09-15 23:57:58\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - Number of files is 2, source profile {'max_file_size': 0.011109352111816406, 'min_file_size': 0.010775566101074219, 'total_file_size': 0.021884918212890625}\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - Cluster resources: {'cpus': 16, 'gpus': 1, 'memory': 7.21808166615665, 'object_store': 3.609040832147002}\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - Fuzzy: num buckets 5, bucket length 11\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - Table preprocessing uses 1 readers\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:57:58 INFO - created 1 table processor actors\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:01 INFO - Completed 1 files in 0.047 min\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:01 INFO - Completed 1 files (50.0%)  in 0.047 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:10 INFO - Completed processing 2 files in 0.197 min\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:10 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:11 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:11 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:12 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:12 INFO - created 1 document actors\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:12 INFO - created 1 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:12 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:12 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=577064)\u001b[0m 23:58:12 INFO - processing buckets 0 long, 10 short\n",
      "\u001b[36m(BucketsHash pid=577064)\u001b[0m 23:58:12 INFO - Done submitting long buckets\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=577732)\u001b[0m 23:58:13 INFO - Waiting bucket processing completion. Submitted requests 1\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:13 INFO - Done processing buckets in 0.01 min\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:13 INFO - creating document snapshots\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:14 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:14 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:18 INFO - Completed processing 2 files in 0.064 min\n",
      "\u001b[36m(orchestrate pid=576219)\u001b[0m 23:58:18 INFO - done flushing in 0.002 sec\n",
      "23:58:28 INFO - Completed execution in 0.543 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:5 completed successfully\n",
      "CPU times: user 511 ms, sys: 392 ms, total: 904 ms\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.3,\n",
    "    \"fdedup_doc_cpu\": 0.3,\n",
    "    \"fdedup_mhash_cpu\": 0.3,\n",
    "    \"fdedup_num_doc_actors\": 1,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 1,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8cd11",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e899ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (2, 16)\n",
      "Output data dimensions (rows x columns)=  (2, 18)\n",
      "Duplicate chunks removed  by fuzzy-dedupe:   0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>removed</th>\n",
       "      <th>int_id_column</th>\n",
       "      <th>hash_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>82</td>\n",
       "      <td>1158</td>\n",
       "      <td>63d14368-4cbc-46e7-aede-f103cfdc5b5b</td>\n",
       "      <td>pdf</td>\n",
       "      <td>49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...</td>\n",
       "      <td>1255781</td>\n",
       "      <td>2024-09-15T23:54:23.967489</td>\n",
       "      <td>106.661048</td>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>A message from our CEO\\n\"We started this new y...</td>\n",
       "      <td>$.main-text[6]</td>\n",
       "      <td>2</td>\n",
       "      <td>[29.09381294, 166.47808838, 500.76074219, 270....</td>\n",
       "      <td>[63d14368-4cbc-46e7-aede-f103cfdc5b5b, 63d1436...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>d0f0c84e-8b9f-4c01-a1a5-65d3c675400e</td>\n",
       "      <td>pdf</td>\n",
       "      <td>29190b8923fb2153321dc52292e19b359a96c2e9974048...</td>\n",
       "      <td>1215216</td>\n",
       "      <td>2024-09-15T23:56:08.895449</td>\n",
       "      <td>104.766304</td>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>A message from our CEO\\n\"At Walmart, we're a p...</td>\n",
       "      <td>$.main-text[6]</td>\n",
       "      <td>2</td>\n",
       "      <td>[30.18040848, 338.63272095, 526.46081543, 436....</td>\n",
       "      <td>[d0f0c84e-8b9f-4c01-a1a5-65d3c675400e, d0f0c84...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  num_pages  num_tables  \\\n",
       "0  Walmart-10K-Reports-Optimized_2023.pdf        100          82   \n",
       "1                        Walmart_2024.pdf        100          83   \n",
       "\n",
       "   num_doc_elements                           document_id  ext  \\\n",
       "0              1158  63d14368-4cbc-46e7-aede-f103cfdc5b5b  pdf   \n",
       "1              1058  d0f0c84e-8b9f-4c01-a1a5-65d3c675400e  pdf   \n",
       "\n",
       "                                                hash     size  \\\n",
       "0  49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...  1255781   \n",
       "1  29190b8923fb2153321dc52292e19b359a96c2e9974048...  1215216   \n",
       "\n",
       "                date_acquired  pdf_convert_time  \\\n",
       "0  2024-09-15T23:54:23.967489        106.661048   \n",
       "1  2024-09-15T23:56:08.895449        104.766304   \n",
       "\n",
       "                          source_filename  \\\n",
       "0  Walmart-10K-Reports-Optimized_2023.pdf   \n",
       "1                        Walmart_2024.pdf   \n",
       "\n",
       "                                            contents    doc_jsonpath  \\\n",
       "0  A message from our CEO\\n\"We started this new y...  $.main-text[6]   \n",
       "1  A message from our CEO\\n\"At Walmart, we're a p...  $.main-text[6]   \n",
       "\n",
       "   page_number                                               bbox  \\\n",
       "0            2  [29.09381294, 166.47808838, 500.76074219, 270....   \n",
       "1            2  [30.18040848, 338.63272095, 526.46081543, 436....   \n",
       "\n",
       "                                             removed  int_id_column  \\\n",
       "0  [63d14368-4cbc-46e7-aede-f103cfdc5b5b, 63d1436...              0   \n",
       "1  [d0f0c84e-8b9f-4c01-a1a5-65d3c675400e, d0f0c84...              1   \n",
       "\n",
       "   hash_column  \n",
       "0           -1  \n",
       "1           -1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (\"Duplicate chunks removed  by fuzzy-dedupe:  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646cbb7-3046-44c0-827d-d102d3ff7cb8",
   "metadata": {},
   "source": [
    "## Step-7: Document Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e985668-848b-4633-b0d8-9fe70ada0c91",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f080011-c9fe-430e-9ecc-f2220d2c8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-6: Processing input='output/05_fuzzy_dedupe_out' --> output='output/06_doc_quality_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  = 6\n",
    "\n",
    "input_folder = output_fuzzy_dedupe_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_doc_quality_dir\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02982c5-f398-4a1a-a9fe-42d7ae748c7c",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29319fb9-b0d8-4f86-9bc5-b92960ad8ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:58:29 INFO - doc_quality parameters are : {'text_lang': 'en', 'doc_content_column': 'contents', 'bad_word_filepath': '/home/sujee/my-stuff/projects/ai-alliance/data-prep-kit-sujee-dev/transforms/language/doc_quality/python/ldnoobw/en', 's3_cred': None, 'docq_data_factory': <data_processing.data_access.data_access_factory.DataAccessFactory object at 0x7092cf227490>}\n",
      "23:58:29 INFO - data factory docq_ is using local configuration without input/output path\n",
      "23:58:29 INFO - data factory docq_ max_files -1, n_sample -1\n",
      "23:58:29 INFO - data factory docq_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:58:29 INFO - pipeline id pipeline_id\n",
      "23:58:29 INFO - code location None\n",
      "23:58:29 INFO - data factory data_ is using local data access: input_folder - output/05_fuzzy_dedupe_out output_folder - output/06_doc_quality_out\n",
      "23:58:29 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:58:29 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:58:29 INFO - orchestrator docq started at 2024-09-15 23:58:29\n",
      "23:58:29 INFO - Number of files is 2, source profile {'max_file_size': 0.010804176330566406, 'min_file_size': 0.010473251342773438, 'total_file_size': 0.021277427673339844}\n",
      "23:58:29 INFO - Load badwords found locally from /home/sujee/my-stuff/projects/ai-alliance/data-prep-kit-sujee-dev/transforms/language/doc_quality/python/ldnoobw/en\n",
      "23:58:29 INFO - Completed 1 files (50.0%) in 0.0 min\n",
      "23:58:29 INFO - Completed 2 files (100.0%) in 0.0 min\n",
      "23:58:29 INFO - Done processing 2 files, waiting for flush() completion.\n",
      "23:58:29 INFO - done flushing in 0.0 sec\n",
      "23:58:29 INFO - Completed execution in 0.0 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:6 completed successfully\n",
      "CPU times: user 30.8 ms, sys: 3.01 ms, total: 33.8 ms\n",
      "Wall time: 29.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from doc_quality_transform import (\n",
    "    text_lang_cli_param,\n",
    "    doc_content_column_cli_param,\n",
    "    bad_word_filepath_cli_param,\n",
    ")\n",
    "from doc_quality_transform_python import DocQualityPythonTransformConfiguration\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "doc_quality_basedir = os.path.join(rootdir, \"transforms\", \"language\", \"doc_quality\", \"python\")\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # doc quality configuration\n",
    "    text_lang_cli_param: \"en\",\n",
    "    doc_content_column_cli_param: \"contents\",\n",
    "    bad_word_filepath_cli_param: os.path.join(doc_quality_basedir, \"ldnoobw\", \"en\"),\n",
    "}\n",
    "\n",
    "\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = PythonTransformLauncher(DocQualityPythonTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7d855",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f631d5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (2, 16)\n",
      "Output data dimensions (rows x columns)=  (2, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>...</th>\n",
       "      <th>docq_mean_word_len</th>\n",
       "      <th>docq_symbol_to_word_ratio</th>\n",
       "      <th>docq_sentence_count</th>\n",
       "      <th>docq_lorem_ipsum_ratio</th>\n",
       "      <th>docq_curly_bracket_ratio</th>\n",
       "      <th>docq_contain_bad_word</th>\n",
       "      <th>docq_bullet_point_ratio</th>\n",
       "      <th>docq_ellipsis_line_ratio</th>\n",
       "      <th>docq_alphabet_word_ratio</th>\n",
       "      <th>docq_contain_common_en_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>82</td>\n",
       "      <td>1158</td>\n",
       "      <td>63d14368-4cbc-46e7-aede-f103cfdc5b5b</td>\n",
       "      <td>pdf</td>\n",
       "      <td>49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...</td>\n",
       "      <td>1255781</td>\n",
       "      <td>2024-09-15T23:54:23.967489</td>\n",
       "      <td>106.661048</td>\n",
       "      <td>...</td>\n",
       "      <td>4.83871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>d0f0c84e-8b9f-4c01-a1a5-65d3c675400e</td>\n",
       "      <td>pdf</td>\n",
       "      <td>29190b8923fb2153321dc52292e19b359a96c2e9974048...</td>\n",
       "      <td>1215216</td>\n",
       "      <td>2024-09-15T23:56:08.895449</td>\n",
       "      <td>104.766304</td>\n",
       "      <td>...</td>\n",
       "      <td>5.16000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  num_pages  num_tables  \\\n",
       "0  Walmart-10K-Reports-Optimized_2023.pdf        100          82   \n",
       "1                        Walmart_2024.pdf        100          83   \n",
       "\n",
       "   num_doc_elements                           document_id  ext  \\\n",
       "0              1158  63d14368-4cbc-46e7-aede-f103cfdc5b5b  pdf   \n",
       "1              1058  d0f0c84e-8b9f-4c01-a1a5-65d3c675400e  pdf   \n",
       "\n",
       "                                                hash     size  \\\n",
       "0  49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...  1255781   \n",
       "1  29190b8923fb2153321dc52292e19b359a96c2e9974048...  1215216   \n",
       "\n",
       "                date_acquired  pdf_convert_time  ... docq_mean_word_len  \\\n",
       "0  2024-09-15T23:54:23.967489        106.661048  ...            4.83871   \n",
       "1  2024-09-15T23:56:08.895449        104.766304  ...            5.16000   \n",
       "\n",
       "  docq_symbol_to_word_ratio docq_sentence_count  docq_lorem_ipsum_ratio  \\\n",
       "0                       0.0                   3                     0.0   \n",
       "1                       0.0                   4                     0.0   \n",
       "\n",
       "  docq_curly_bracket_ratio docq_contain_bad_word  docq_bullet_point_ratio  \\\n",
       "0                      0.0                 False                      0.0   \n",
       "1                      0.0                 False                      0.0   \n",
       "\n",
       "   docq_ellipsis_line_ratio  docq_alphabet_word_ratio  \\\n",
       "0                       0.0                       1.0   \n",
       "1                       0.0                       1.0   \n",
       "\n",
       "   docq_contain_common_en_words  \n",
       "0                          True  \n",
       "1                          True  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## Step-8:   Text encoding\n",
    "\n",
    "Encode text for the vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-7: Processing input='output/06_doc_quality_out' --> output='output/07_embeddings_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  = 7\n",
    "\n",
    "input_folder = output_doc_quality_dir\n",
    "output_folder =  output_embeddings_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:58:30 INFO - text_encoder parameters are : {'content_column_name': 'contents', 'output_embeddings_column_name': 'embeddings', 'model_name': 'sentence-transformers/all-MiniLM-L6-v2'}\n",
      "23:58:30 INFO - pipeline id pipeline_id\n",
      "23:58:30 INFO - code location None\n",
      "23:58:30 INFO - data factory data_ is using local data access: input_folder - output/06_doc_quality_out output_folder - output/07_embeddings_out\n",
      "23:58:30 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:58:30 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:58:30 INFO - orchestrator text_encoder started at 2024-09-15 23:58:30\n",
      "23:58:30 INFO - Number of files is 2, source profile {'max_file_size': 0.015343666076660156, 'min_file_size': 0.01500701904296875, 'total_file_size': 0.030350685119628906}\n",
      "23:58:31 INFO - Completed 1 files (50.0%) in 0.001 min\n",
      "23:58:31 INFO - Completed 2 files (100.0%) in 0.001 min\n",
      "23:58:31 INFO - Done processing 2 files, waiting for flush() completion.\n",
      "23:58:31 INFO - done flushing in 0.0 sec\n",
      "23:58:31 INFO - Completed execution in 0.032 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:7 completed successfully\n",
      "CPU times: user 491 ms, sys: 151 ms, total: 642 ms\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from text_encoder_transform_python import TextEncoderPythonTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # text_encoder\n",
    "    \"text_encoder_model_name\": MY_CONFIG.EMBEDDING_MODEL,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = PythonTransformLauncher(TextEncoderPythonTransformConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734852c",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b1c1d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (2, 29)\n",
      "Output data dimensions (rows x columns)=  (2, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>...</th>\n",
       "      <th>docq_symbol_to_word_ratio</th>\n",
       "      <th>docq_sentence_count</th>\n",
       "      <th>docq_lorem_ipsum_ratio</th>\n",
       "      <th>docq_curly_bracket_ratio</th>\n",
       "      <th>docq_contain_bad_word</th>\n",
       "      <th>docq_bullet_point_ratio</th>\n",
       "      <th>docq_ellipsis_line_ratio</th>\n",
       "      <th>docq_alphabet_word_ratio</th>\n",
       "      <th>docq_contain_common_en_words</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart_2024.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>83</td>\n",
       "      <td>1058</td>\n",
       "      <td>d0f0c84e-8b9f-4c01-a1a5-65d3c675400e</td>\n",
       "      <td>pdf</td>\n",
       "      <td>29190b8923fb2153321dc52292e19b359a96c2e9974048...</td>\n",
       "      <td>1215216</td>\n",
       "      <td>2024-09-15T23:56:08.895449</td>\n",
       "      <td>104.766304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.007453779, 0.08018814, 0.051178806, 0.00213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart-10K-Reports-Optimized_2023.pdf</td>\n",
       "      <td>100</td>\n",
       "      <td>82</td>\n",
       "      <td>1158</td>\n",
       "      <td>63d14368-4cbc-46e7-aede-f103cfdc5b5b</td>\n",
       "      <td>pdf</td>\n",
       "      <td>49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...</td>\n",
       "      <td>1255781</td>\n",
       "      <td>2024-09-15T23:54:23.967489</td>\n",
       "      <td>106.661048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.014166234, 0.07892205, 0.068165675, -0.0189...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  num_pages  num_tables  \\\n",
       "1                        Walmart_2024.pdf        100          83   \n",
       "0  Walmart-10K-Reports-Optimized_2023.pdf        100          82   \n",
       "\n",
       "   num_doc_elements                           document_id  ext  \\\n",
       "1              1058  d0f0c84e-8b9f-4c01-a1a5-65d3c675400e  pdf   \n",
       "0              1158  63d14368-4cbc-46e7-aede-f103cfdc5b5b  pdf   \n",
       "\n",
       "                                                hash     size  \\\n",
       "1  29190b8923fb2153321dc52292e19b359a96c2e9974048...  1215216   \n",
       "0  49138b69e8da8581e155885fb487ff96e9fe2fa24e7e6c...  1255781   \n",
       "\n",
       "                date_acquired  pdf_convert_time  ...  \\\n",
       "1  2024-09-15T23:56:08.895449        104.766304  ...   \n",
       "0  2024-09-15T23:54:23.967489        106.661048  ...   \n",
       "\n",
       "  docq_symbol_to_word_ratio docq_sentence_count docq_lorem_ipsum_ratio  \\\n",
       "1                       0.0                   4                    0.0   \n",
       "0                       0.0                   3                    0.0   \n",
       "\n",
       "   docq_curly_bracket_ratio docq_contain_bad_word docq_bullet_point_ratio  \\\n",
       "1                       0.0                 False                     0.0   \n",
       "0                       0.0                 False                     0.0   \n",
       "\n",
       "   docq_ellipsis_line_ratio  docq_alphabet_word_ratio  \\\n",
       "1                       0.0                       1.0   \n",
       "0                       0.0                       1.0   \n",
       "\n",
       "   docq_contain_common_en_words  \\\n",
       "1                          True   \n",
       "0                          True   \n",
       "\n",
       "                                          embeddings  \n",
       "1  [0.007453779, 0.08018814, 0.051178806, 0.00213...  \n",
       "0  [0.014166234, 0.07892205, 0.068165675, -0.0189...  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12630-be6b-4188-a925-77117155617b",
   "metadata": {},
   "source": [
    "## Step-9: Copy output to final output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Copied output from 'output/07_embeddings_out' --> 'output/output_final'\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER_FINAL, ignore_errors=True)\n",
    "shutil.copytree(src=output_folder, dst=MY_CONFIG.OUTPUT_FOLDER_FINAL)\n",
    "\n",
    "print (f\"‚úÖ Copied output from '{output_folder}' --> '{MY_CONFIG.OUTPUT_FOLDER_FINAL}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce85f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
